{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14902d5b-2990-403f-b37b-6c3bfecc054e",
   "metadata": {},
   "source": [
    "# Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813fcdf-bed9-40c6-88fc-22cea748aceb",
   "metadata": {},
   "source": [
    "# Web scraping - \n",
    "\n",
    "1. Web scraping is an automatic method to obtain large amounts of data from websites. \n",
    "2. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. \n",
    "3. There are many different ways to perform web scraping to obtain data from websites. \n",
    "4. These include using online services, particular API’s or even creating your code for web scraping from scratch. \n",
    "5. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. \n",
    "6. This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced.\n",
    "7. In that situation, it’s best to use Web Scraping to scrape the website for data.\n",
    "\n",
    "What is Web Scraping used for?\n",
    "\n",
    "1. Price Monitoring\n",
    "Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. \n",
    "Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n",
    "\n",
    "2. Market Research\n",
    "Web scraping can be used for market research by companies. \n",
    "High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future. \n",
    "\n",
    "3. News Monitoring\n",
    "Web scraping news sites can provide detailed reports on the current news to a company. \n",
    "This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!\n",
    "\n",
    "4. Sentiment Analysis\n",
    "If companies want to understand the general sentiment for their products among their consumers, then Sentiment Analysis is a must. \n",
    "Companies can use web scraping to collect data from social media websites such as Facebook and Twitter as to what the general sentiment about their products is. This will help them in creating products that people desire and moving ahead of their competition.\n",
    "\n",
    "5. Email Marketing\n",
    "Companies can also use Web scraping for email marketing. \n",
    "They can collect Email ID’s from various sites using web scraping and then send bulk promotional and marketing Emails to all the people owning these Email ID’s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c528f3c1-159a-470e-afab-aa7aa115e8a5",
   "metadata": {},
   "source": [
    "# Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35150f-a1af-4916-b2a7-016d75c1fb3b",
   "metadata": {},
   "source": [
    "1. HTML Parsing\n",
    "HTML parsing involves the use of JavaScript to target a linear or nested HTML page. \n",
    "It is a powerful and fast method for extracting text and links (e.g. a nested link or email address), scraping screens and pulling resources.\n",
    "\n",
    "2. DOM Parsing\n",
    "The Document Object Model (DOM) defines the structure, style and content of an XML file. \n",
    "Scrapers typically use a DOM parser to view the structure of web pages in depth. \n",
    "DOM parsers can be used to access the nodes that contain information and scrape the web page with tools like XPath. \n",
    "For dynamically generated content, scrapers can embed web browsers like Firefox and Internet Explorer to extract whole web pages (or parts of them).\n",
    "\n",
    "3. Vertical Aggregation\n",
    "Companies that use extensive computing power can create vertical aggregation platforms to target particular verticals. \n",
    "These are data harvesting platforms that can be run on the cloud and are used to automatically generate and monitor bots for certain verticals with minimal human intervention. \n",
    "Bots are generated according to the information required to each vertical, and their efficiency is determined by the quality of data they extract.\n",
    "\n",
    "4. XPath\n",
    "XPath is short for XML Path Language, which is a query language for XML documents. \n",
    "XML documents have tree-like structures, so scrapers can use XPath to navigate through them by selecting nodes according to various parameters. A scraper may combine DOM parsing with XPath to extract whole web pages and publish them on a destination site.\n",
    "\n",
    "5. Google Sheets\n",
    "Google Sheets is a popular tool for data scraping. Scarpers can use the IMPORTXML function in Sheets to scrape from a website, which is useful if they want to extract a specific pattern or data from the website. \n",
    "This command also makes it possible to check if a website can be scraped or is protected.\n",
    "\n",
    "6. Rate Limit User Requests\n",
    "The rate of interaction for human visitors clicking through a website is relatively predictable. \n",
    "For example, it is impossible for a human to go through 100 web pages per second, while machines can make multiple simultaneous requests. \n",
    "The rate of requests can indicate the use of data scraping techniques that attempt to scrape your entire site in a short time.\n",
    "You can rate limit the number of requests an IP address can make within a particular time frame. \n",
    "This will protect your website from exploitation and significantly slow down the rate at which data scraping can occur.\n",
    "\n",
    "7. Mitigate High-Volume Requesters with CAPTCHAs\n",
    "Another way to slow down data scraping efforts is to apply CAPTCHAs. \n",
    "These require website visitors to complete a task that would be relatively easy for a human but prohibitively challenging for a machine. \n",
    "Even if a bot can get past the CAPTCHA once, it will not be able to do so across multiple instances. \n",
    "The drawback of CAPTCHA challenges is their potential negative impact on user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0ac3f-823e-44cc-8707-a53c9461c19f",
   "metadata": {},
   "source": [
    "# Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a072e2-cad4-4207-9046-3b0fa5c8f71f",
   "metadata": {},
   "source": [
    "1. The Beautiful Soup is a python library which is named after a Lewis Carroll poem of the same name in “Alice’s Adventures in the Wonderland”. \n",
    "2. Beautiful Soup is a python package and as the name suggests, parses the unwanted data and helps to organize and format the messy web data by fixing bad HTML and present to us in an easily-traversible XML structures.\n",
    "3. The Beautiful Soup documentation will give you a sense of variety of things that the Beautiful Soup library will help with, from isolating titles and links, to extracting all of the text from the html tags, to altering the HTML within the document you’re working with.\n",
    "4. Beautiful Soup is a python package and as the name suggests, parses the unwanted data and helps to organize and format the messy web data by fixing bad HTML and present to us in an easily-traversible XML structures. \n",
    "5. In short, Beautiful Soup is a python package which allows us to pull data out of HTML and XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5765a410-0c28-4292-83bf-9825b73e87d1",
   "metadata": {},
   "source": [
    "# Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa42959c-cd1b-418a-b4b7-c5ffd1d20428",
   "metadata": {},
   "source": [
    "1. Flask is an API of Python that allows us to build up web-applications. \n",
    "2. It was developed by Armin Ronacher.\n",
    "3. Flask’s framework is more explicit than Django’s framework and is also easier to learn because it has less base code to implement a simple web-Application. \n",
    "4. A Web-Application Framework or Web Framework is the collection of modules and libraries that helps the developer to write applications without writing the low-level codes such as protocols, thread management, etc. \n",
    "5. Flask is based on WSGI(Web Server Gateway Interface) toolkit and Jinja2 template engine.Flask is used for developing web applications using python, implemented on Werkzeug and Jinja2. \n",
    "6. Advantages of using Flask framework are: \n",
    "7. There is a built-in development server and a fast debugger provided. Lightweight\n",
    "8. Flask is a lightweight framework to build websites.\n",
    "9. We’ll use this to parse our collected data and display it as HTML in a new HTML file.\n",
    "10. The requests module allows us to send http requests to the website we want to scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70814db-7078-43d1-bb18-009c45528023",
   "metadata": {},
   "source": [
    "# Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d25e885-16fd-4490-a043-2a0ad9921892",
   "metadata": {},
   "source": [
    "1. AWS is the largest cloud computing platform with over 200+ featured resources.\n",
    "2. It is a platform that provides a pay-as-you-go service. Gone are the days when you need to install servers and look after maintenance and cost of them. \n",
    "3. AWS offers a “pay-as-you-go” feature where you just have to pay for the services you use and also the time period you use.\n",
    "\n",
    "## AWS services Used in Project :\n",
    "\n",
    "1. Amazon EC2 (Elastic Cloud Compute)\n",
    "Amazon EC2 is the fastest cloud computing service provided by AWS. It offers virtual, secure, reliable, and resizable servers for any workload. \n",
    "Through this service, it becomes easy for developers to access resources and also facilitates web-scale cloud computing. \n",
    "This comes with the best suitable processors, networking facilities, and storage systems.\n",
    "Developers can quickly and dynamically scale capacities as per business needs. \n",
    "It has over 500 instances and you can also choose the latest processor, operating system, storage, and networking to help you choose according to the needs of the business. \n",
    "Also, with Amazon EC2, you only have to pay for what you use, and also as per the time period, scale with amazon EC2 auto-scaling has optimal storage and can optimize CPU configurations.\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
